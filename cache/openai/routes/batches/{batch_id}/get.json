{
  "description": "Retrieves a batch.",
  "operationId": "retrieveBatch",
  "parameters": [
    {
      "description": "The ID of the batch to retrieve.",
      "in": "path",
      "name": "batch_id",
      "required": true,
      "schema": {
        "type": "string"
      }
    }
  ],
  "responses": {
    "200": {
      "content": {
        "application/json": {
          "schema": {
            "properties": {
              "cancelled_at": {
                "description": "The Unix timestamp (in seconds) for when the batch was cancelled.",
                "type": "integer"
              },
              "cancelling_at": {
                "description": "The Unix timestamp (in seconds) for when the batch started cancelling.",
                "type": "integer"
              },
              "completed_at": {
                "description": "The Unix timestamp (in seconds) for when the batch was completed.",
                "type": "integer"
              },
              "completion_window": {
                "description": "The time frame within which the batch should be processed.",
                "type": "string"
              },
              "created_at": {
                "description": "The Unix timestamp (in seconds) for when the batch was created.",
                "type": "integer"
              },
              "endpoint": {
                "description": "The OpenAI API endpoint used by the batch.",
                "type": "string"
              },
              "error_file_id": {
                "description": "The ID of the file containing the outputs of requests with errors.",
                "type": "string"
              },
              "errors": {
                "properties": {
                  "data": {
                    "items": {
                      "properties": {
                        "code": {
                          "description": "An error code identifying the error type.",
                          "type": "string"
                        },
                        "line": {
                          "anyOf": [
                            {
                              "description": "The line number of the input file where the error occurred, if applicable.",
                              "type": "integer"
                            },
                            {
                              "type": "null"
                            }
                          ]
                        },
                        "message": {
                          "description": "A human-readable message providing more details about the error.",
                          "type": "string"
                        },
                        "param": {
                          "anyOf": [
                            {
                              "description": "The name of the parameter that caused the error, if applicable.",
                              "type": "string"
                            },
                            {
                              "type": "null"
                            }
                          ]
                        }
                      },
                      "type": "object"
                    },
                    "type": "array"
                  },
                  "object": {
                    "description": "The object type, which is always `list`.",
                    "type": "string"
                  }
                },
                "type": "object"
              },
              "expired_at": {
                "description": "The Unix timestamp (in seconds) for when the batch expired.",
                "type": "integer"
              },
              "expires_at": {
                "description": "The Unix timestamp (in seconds) for when the batch will expire.",
                "type": "integer"
              },
              "failed_at": {
                "description": "The Unix timestamp (in seconds) for when the batch failed.",
                "type": "integer"
              },
              "finalizing_at": {
                "description": "The Unix timestamp (in seconds) for when the batch started finalizing.",
                "type": "integer"
              },
              "id": {
                "type": "string"
              },
              "in_progress_at": {
                "description": "The Unix timestamp (in seconds) for when the batch started processing.",
                "type": "integer"
              },
              "input_file_id": {
                "description": "The ID of the input file for the batch.",
                "type": "string"
              },
              "metadata": {
                "anyOf": [
                  {
                    "additionalProperties": {
                      "type": "string"
                    },
                    "description": "Set of 16 key-value pairs that can be attached to an object. This can be\nuseful for storing additional information about the object in a structured\nformat, and querying for objects via API or the dashboard.\n\nKeys are strings with a maximum length of 64 characters. Values are strings\nwith a maximum length of 512 characters.\n",
                    "type": "object"
                  },
                  {
                    "type": "null"
                  }
                ]
              },
              "model": {
                "description": "Model ID used to process the batch, like `gpt-5-2025-08-07`. OpenAI\noffers a wide range of models with different capabilities, performance\ncharacteristics, and price points. Refer to the [model\nguide](https://platform.openai.com/docs/models) to browse and compare available models.\n",
                "type": "string"
              },
              "object": {
                "description": "The object type, which is always `batch`.",
                "enum": [
                  "batch"
                ],
                "type": "string"
              },
              "output_file_id": {
                "description": "The ID of the file containing the outputs of successfully executed requests.",
                "type": "string"
              },
              "request_counts": {
                "description": "The request counts for different statuses within the batch.",
                "properties": {
                  "completed": {
                    "description": "Number of requests that have been completed successfully.",
                    "type": "integer"
                  },
                  "failed": {
                    "description": "Number of requests that have failed.",
                    "type": "integer"
                  },
                  "total": {
                    "description": "Total number of requests in the batch.",
                    "type": "integer"
                  }
                },
                "required": [
                  "total",
                  "completed",
                  "failed"
                ],
                "type": "object"
              },
              "status": {
                "description": "The current status of the batch.",
                "enum": [
                  "validating",
                  "failed",
                  "in_progress",
                  "finalizing",
                  "completed",
                  "expired",
                  "cancelling",
                  "cancelled"
                ],
                "type": "string"
              },
              "usage": {
                "description": "Represents token usage details including input tokens, output tokens, a\nbreakdown of output tokens, and the total tokens used. Only populated on\nbatches created after September 7, 2025.\n",
                "properties": {
                  "input_tokens": {
                    "description": "The number of input tokens.",
                    "type": "integer"
                  },
                  "input_tokens_details": {
                    "description": "A detailed breakdown of the input tokens.",
                    "properties": {
                      "cached_tokens": {
                        "description": "The number of tokens that were retrieved from the cache. [More on\nprompt caching](https://platform.openai.com/docs/guides/prompt-caching).\n",
                        "type": "integer"
                      }
                    },
                    "required": [
                      "cached_tokens"
                    ],
                    "type": "object"
                  },
                  "output_tokens": {
                    "description": "The number of output tokens.",
                    "type": "integer"
                  },
                  "output_tokens_details": {
                    "description": "A detailed breakdown of the output tokens.",
                    "properties": {
                      "reasoning_tokens": {
                        "description": "The number of reasoning tokens.",
                        "type": "integer"
                      }
                    },
                    "required": [
                      "reasoning_tokens"
                    ],
                    "type": "object"
                  },
                  "total_tokens": {
                    "description": "The total number of tokens used.",
                    "type": "integer"
                  }
                },
                "required": [
                  "input_tokens",
                  "input_tokens_details",
                  "output_tokens",
                  "output_tokens_details",
                  "total_tokens"
                ],
                "type": "object"
              }
            },
            "required": [
              "id",
              "object",
              "endpoint",
              "input_file_id",
              "completion_window",
              "status",
              "created_at"
            ],
            "type": "object"
          }
        }
      },
      "description": "Batch retrieved successfully."
    }
  },
  "summary": "Retrieve batch",
  "tags": [
    "Batch"
  ]
}
