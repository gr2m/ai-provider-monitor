{
  "operationId": "StreamingController_getStreamingJobs_v2",
  "parameters": [
    {
      "description": "The starting point for pagination. A value of 0 starts from the first item.",
      "in": "query",
      "name": "offset",
      "required": false,
      "schema": {
        "default": 0,
        "minimum": 0,
        "type": "integer"
      }
    },
    {
      "description": "The maximum number of items to return. Useful for pagination and controlling data payload size.",
      "in": "query",
      "name": "limit",
      "required": false,
      "schema": {
        "default": 20,
        "minimum": 1,
        "type": "integer"
      }
    },
    {
      "description": "Filter items relevant to a specific date in ISO format (YYYY-MM-DD).",
      "in": "query",
      "name": "date",
      "required": false,
      "schema": {
        "example": "2026-02-28",
        "format": "date-time",
        "type": "string"
      }
    },
    {
      "description": "Include items that occurred before the specified date in ISO format.",
      "in": "query",
      "name": "before_date",
      "required": false,
      "schema": {
        "example": "2026-02-28T00:00:10.663Z",
        "format": "date-time",
        "type": "string"
      }
    },
    {
      "description": "Filter for items after the specified date. Use with `before_date` for a range. Date in ISO format.",
      "in": "query",
      "name": "after_date",
      "required": false,
      "schema": {
        "example": "2026-02-28T00:00:10.663Z",
        "format": "date-time",
        "type": "string"
      }
    },
    {
      "description": "Filter the list based on item status. Accepts multiple values from the predefined list.",
      "in": "query",
      "name": "status",
      "required": false,
      "schema": {
        "example": [
          "done"
        ],
        "items": {
          "enum": [
            "queued",
            "processing",
            "done",
            "error"
          ],
          "type": "string"
        },
        "type": "array"
      }
    },
    {
      "in": "query",
      "name": "custom_metadata",
      "required": false,
      "schema": {
        "additionalProperties": true,
        "example": {
          "user": "John Doe"
        },
        "type": "object"
      }
    }
  ],
  "responses": {
    "200": {
      "content": {
        "application/json": {
          "schema": {
            "$ref": "#/components/schemas/ListStreamingResponse"
          }
        }
      },
      "description": "A list of live jobs matching the parameters."
    },
    "401": {
      "content": {
        "application/json": {
          "schema": {
            "$ref": "#/components/schemas/UnauthorizedErrorResponse"
          }
        }
      },
      "description": "You don't have the permissions to access live jobs"
    }
  },
  "security": [
    {
      "x_gladia_key": []
    }
  ],
  "summary": "Get live jobs based on query parameters",
  "tags": [
    "Live V2"
  ],
  "components": {
    "schemas": {
      "ListStreamingResponse": {
        "properties": {
          "current": {
            "description": "URL to fetch the current page",
            "example": "https://api.gladia.io/v2/transcription?status=done&offset=0&limit=20",
            "format": "uri",
            "type": "string"
          },
          "first": {
            "description": "URL to fetch the first page",
            "example": "https://api.gladia.io/v2/transcription?status=done&offset=0&limit=20",
            "format": "uri",
            "type": "string"
          },
          "items": {
            "description": "List of live transcriptions",
            "items": {
              "$ref": "#/components/schemas/StreamingResponse"
            },
            "type": "array"
          },
          "next": {
            "description": "URL to fetch the next page",
            "example": "https://api.gladia.io/v2/transcription?status=done&offset=20&limit=20",
            "format": "uri",
            "nullable": true,
            "type": "string"
          }
        },
        "required": [
          "first",
          "current",
          "next",
          "items"
        ],
        "type": "object"
      },
      "UnauthorizedErrorResponse": {
        "properties": {
          "message": {
            "description": "Error message",
            "example": "gladia key not found",
            "type": "string"
          },
          "path": {
            "description": "Path to the API endpoint",
            "example": "/v2/transcription/45463597-20b7-4af7-b3b3-f5fb778203ab",
            "type": "string"
          },
          "request_id": {
            "description": "Debug id",
            "example": "G-821fe9df",
            "type": "string"
          },
          "statusCode": {
            "description": "HTTP status code of the error",
            "example": 401,
            "type": "number"
          },
          "timestamp": {
            "description": "Date of when the error occurred",
            "example": "2023-12-28T09:04:17.210Z",
            "type": "string"
          }
        },
        "required": [
          "timestamp",
          "path",
          "request_id",
          "statusCode",
          "message"
        ],
        "type": "object"
      },
      "StreamingResponse": {
        "properties": {
          "completed_at": {
            "description": "Completion date when status is \"done\" or \"error\"",
            "example": "2023-12-28T09:04:37.210Z",
            "format": "date-time",
            "nullable": true,
            "type": "string"
          },
          "created_at": {
            "description": "Creation date",
            "example": "2023-12-28T09:04:17.210Z",
            "format": "date-time",
            "type": "string"
          },
          "custom_metadata": {
            "additionalProperties": true,
            "description": "Custom metadata given in the initial request",
            "example": {
              "user": "John Doe"
            },
            "type": "object"
          },
          "error_code": {
            "description": "HTTP status code of the error if status is \"error\"",
            "example": 500,
            "maximum": 599,
            "minimum": 400,
            "nullable": true,
            "type": "integer"
          },
          "file": {
            "allOf": [
              {
                "$ref": "#/components/schemas/FileResponse"
              }
            ],
            "description": "The file data you uploaded. Can be null if status is \"error\"",
            "nullable": true
          },
          "id": {
            "description": "Id of the job",
            "example": "45463597-20b7-4af7-b3b3-f5fb778203ab",
            "format": "uuid",
            "type": "string"
          },
          "kind": {
            "default": "live",
            "enum": [
              "live"
            ],
            "example": "live",
            "type": "string"
          },
          "post_session_metadata": {
            "description": "For debugging purposes, send data that could help to identify issues",
            "type": "object"
          },
          "request_id": {
            "description": "Debug id",
            "example": "G-45463597",
            "type": "string"
          },
          "request_params": {
            "allOf": [
              {
                "$ref": "#/components/schemas/StreamingRequestParamsResponse"
              }
            ],
            "description": "Parameters used for this live transcription. Can be null if status is \"error\"",
            "nullable": true
          },
          "result": {
            "allOf": [
              {
                "$ref": "#/components/schemas/StreamingTranscriptionResultWithMessagesDTO"
              }
            ],
            "description": "Live transcription's result when status is \"done\"",
            "nullable": true
          },
          "status": {
            "description": "\"queued\": the job has been queued. \"processing\": the job is being processed. \"done\": the job has been processed and the result is available. \"error\": an error occurred during the job's processing.",
            "enum": [
              "queued",
              "processing",
              "done",
              "error"
            ],
            "type": "string"
          },
          "version": {
            "description": "API version",
            "example": 2,
            "type": "integer"
          }
        },
        "required": [
          "id",
          "request_id",
          "version",
          "status",
          "created_at",
          "post_session_metadata",
          "kind"
        ],
        "type": "object"
      },
      "FileResponse": {
        "properties": {
          "audio_duration": {
            "description": "Duration of the audio file",
            "example": 3600,
            "nullable": true,
            "type": "number"
          },
          "filename": {
            "description": "The name of the uploaded file",
            "nullable": true,
            "type": "string"
          },
          "id": {
            "description": "The file id",
            "type": "string"
          },
          "number_of_channels": {
            "description": "Number of channels in the audio file",
            "example": 1,
            "minimum": 1,
            "nullable": true,
            "type": "integer"
          },
          "source": {
            "description": "The link used to download the file if audio_url was used",
            "nullable": true,
            "type": "string"
          }
        },
        "required": [
          "id",
          "filename",
          "source",
          "audio_duration",
          "number_of_channels"
        ],
        "type": "object"
      },
      "StreamingRequestParamsResponse": {
        "properties": {
          "bit_depth": {
            "allOf": [
              {
                "$ref": "#/components/schemas/StreamingSupportedBitDepthEnum"
              }
            ],
            "default": 16,
            "description": "The bit depth of the audio stream"
          },
          "callback": {
            "default": false,
            "description": "If true, messages will be sent to configured url.",
            "type": "boolean"
          },
          "callback_config": {
            "allOf": [
              {
                "$ref": "#/components/schemas/CallbackConfig"
              }
            ],
            "description": "Specify the callback configuration"
          },
          "channels": {
            "default": 1,
            "description": "The number of channels of the audio stream",
            "maximum": 8,
            "minimum": 1,
            "type": "integer"
          },
          "encoding": {
            "allOf": [
              {
                "$ref": "#/components/schemas/StreamingSupportedEncodingEnum"
              }
            ],
            "default": "wav/pcm",
            "description": "The encoding format of the audio stream. Supported formats: \n- PCM: 8, 16, 24, and 32 bits \n- A-law: 8 bits \n- μ-law: 8 bits \n\nNote: No need to add WAV headers to raw audio as the API supports both formats."
          },
          "endpointing": {
            "default": 0.05,
            "description": "The endpointing duration in seconds. Endpointing is the duration of silence which will cause an utterance to be considered as finished",
            "maximum": 10,
            "minimum": 0.01,
            "type": "number"
          },
          "language_config": {
            "allOf": [
              {
                "$ref": "#/components/schemas/LanguageConfig"
              }
            ],
            "description": "Specify the language configuration"
          },
          "maximum_duration_without_endpointing": {
            "default": 5,
            "description": "The maximum duration in seconds without endpointing. If endpointing is not detected after this duration, current utterance will be considered as finished",
            "maximum": 60,
            "minimum": 5,
            "type": "number"
          },
          "messages_config": {
            "allOf": [
              {
                "$ref": "#/components/schemas/MessagesConfig"
              }
            ],
            "description": "Specify the websocket messages configuration"
          },
          "model": {
            "allOf": [
              {
                "$ref": "#/components/schemas/StreamingSupportedModels"
              }
            ],
            "default": "solaria-1",
            "description": "The model used to process the audio. \"solaria-1\" is used by default."
          },
          "post_processing": {
            "allOf": [
              {
                "$ref": "#/components/schemas/PostProcessingConfig"
              }
            ],
            "description": "Specify the post-processing configuration"
          },
          "pre_processing": {
            "allOf": [
              {
                "$ref": "#/components/schemas/PreProcessingConfig"
              }
            ],
            "description": "Specify the pre-processing configuration"
          },
          "realtime_processing": {
            "allOf": [
              {
                "$ref": "#/components/schemas/RealtimeProcessingConfig"
              }
            ],
            "description": "Specify the realtime processing configuration"
          },
          "sample_rate": {
            "allOf": [
              {
                "$ref": "#/components/schemas/StreamingSupportedSampleRateEnum"
              }
            ],
            "default": 16000,
            "description": "The sample rate of the audio stream"
          }
        },
        "type": "object"
      },
      "StreamingTranscriptionResultWithMessagesDTO": {
        "properties": {
          "chapterization": {
            "allOf": [
              {
                "$ref": "#/components/schemas/ChapterizationDTO"
              }
            ],
            "description": "If `chapterization` has been enabled, will generate chapters name for different parts of the given audio."
          },
          "messages": {
            "description": "Real-Time messages sent by the server during the live transcription",
            "items": {
              "type": "string"
            },
            "type": "array"
          },
          "metadata": {
            "allOf": [
              {
                "$ref": "#/components/schemas/TranscriptionMetadataDTO"
              }
            ],
            "description": "Metadata for the given transcription & audio file"
          },
          "named_entity_recognition": {
            "allOf": [
              {
                "$ref": "#/components/schemas/NamedEntityRecognitionDTO"
              }
            ],
            "description": "If `named_entity_recognition` has been enabled, the detected entities"
          },
          "sentiment_analysis": {
            "allOf": [
              {
                "$ref": "#/components/schemas/SentimentAnalysisDTO"
              }
            ],
            "description": "If `sentiment_analysis` has been enabled, sentiment analysis of the audio speech transcription"
          },
          "summarization": {
            "allOf": [
              {
                "$ref": "#/components/schemas/SummarizationDTO"
              }
            ],
            "description": "If `summarization` has been enabled, summarization of the audio speech transcription"
          },
          "transcription": {
            "allOf": [
              {
                "$ref": "#/components/schemas/TranscriptionDTO"
              }
            ],
            "description": "Transcription of the audio speech"
          },
          "translation": {
            "allOf": [
              {
                "$ref": "#/components/schemas/TranslationDTO"
              }
            ],
            "description": "If `translation` has been enabled, translation of the audio speech transcription"
          }
        },
        "required": [
          "metadata"
        ],
        "type": "object"
      },
      "StreamingSupportedBitDepthEnum": {
        "description": "The bit depth of the audio stream",
        "enum": [
          8,
          16,
          24,
          32
        ],
        "type": "number"
      },
      "CallbackConfig": {
        "properties": {
          "receive_acknowledgments": {
            "default": false,
            "description": "If true, acknowledgments will be sent to the defined callback.",
            "type": "boolean"
          },
          "receive_errors": {
            "default": false,
            "description": "If true, errors will be sent to the defined callback.",
            "type": "boolean"
          },
          "receive_final_transcripts": {
            "default": true,
            "description": "If true, final transcript will be sent to the defined callback.",
            "type": "boolean"
          },
          "receive_lifecycle_events": {
            "default": true,
            "description": "If true, lifecycle events will be sent to the defined callback.",
            "type": "boolean"
          },
          "receive_partial_transcripts": {
            "default": false,
            "description": "If true, partial transcript will be sent to the defined callback.",
            "type": "boolean"
          },
          "receive_post_processing_events": {
            "default": true,
            "description": "If true, post-processing events will be sent to the defined callback.",
            "type": "boolean"
          },
          "receive_pre_processing_events": {
            "default": true,
            "description": "If true, pre-processing events will be sent to the defined callback.",
            "type": "boolean"
          },
          "receive_realtime_processing_events": {
            "default": true,
            "description": "If true, realtime processing events will be sent to the defined callback.",
            "type": "boolean"
          },
          "receive_speech_events": {
            "default": false,
            "description": "If true, begin and end speech events will be sent to the defined callback.",
            "type": "boolean"
          },
          "url": {
            "description": "URL on which we will do a `POST` request with configured messages",
            "example": "https://callback.example",
            "format": "uri",
            "type": "string"
          }
        },
        "type": "object"
      },
      "StreamingSupportedEncodingEnum": {
        "description": "The encoding format of the audio stream. Supported formats: \n- PCM: 8, 16, 24, and 32 bits \n- A-law: 8 bits \n- μ-law: 8 bits \n\nNote: No need to add WAV headers to raw audio as the API supports both formats.",
        "enum": [
          "wav/pcm",
          "wav/alaw",
          "wav/ulaw"
        ],
        "type": "string"
      },
      "LanguageConfig": {
        "properties": {
          "code_switching": {
            "default": false,
            "description": "If true, language will be auto-detected on each utterance. Otherwise, language will be auto-detected on first utterance and then used for the rest of the transcription. If one language is set, this option will be ignored.",
            "type": "boolean"
          },
          "languages": {
            "default": [],
            "description": "If one language is set, it will be used for the transcription. Otherwise, language will be auto-detected by the model.",
            "items": {
              "$ref": "#/components/schemas/TranscriptionLanguageCodeEnum"
            },
            "type": "array"
          }
        },
        "type": "object"
      },
      "MessagesConfig": {
        "properties": {
          "receive_acknowledgments": {
            "default": true,
            "description": "If true, acknowledgments will be sent to websocket.",
            "type": "boolean"
          },
          "receive_errors": {
            "default": true,
            "description": "If true, errors will be sent to websocket.",
            "type": "boolean"
          },
          "receive_final_transcripts": {
            "default": true,
            "description": "If true, final transcript will be sent to websocket.",
            "type": "boolean"
          },
          "receive_lifecycle_events": {
            "default": false,
            "description": "If true, lifecycle events will be sent to websocket.",
            "type": "boolean"
          },
          "receive_partial_transcripts": {
            "default": false,
            "description": "If true, partial transcript will be sent to websocket.",
            "type": "boolean"
          },
          "receive_post_processing_events": {
            "default": true,
            "description": "If true, post-processing events will be sent to websocket.",
            "type": "boolean"
          },
          "receive_pre_processing_events": {
            "default": true,
            "description": "If true, pre-processing events will be sent to websocket.",
            "type": "boolean"
          },
          "receive_realtime_processing_events": {
            "default": true,
            "description": "If true, realtime processing events will be sent to websocket.",
            "type": "boolean"
          },
          "receive_speech_events": {
            "default": true,
            "description": "If true, begin and end speech events will be sent to websocket.",
            "type": "boolean"
          }
        },
        "type": "object"
      },
      "StreamingSupportedModels": {
        "description": "The model used to process the audio. \"solaria-1\" is used by default.",
        "enum": [
          "solaria-1"
        ],
        "type": "string"
      },
      "PostProcessingConfig": {
        "properties": {
          "chapterization": {
            "default": false,
            "description": "If true, generates chapters for the whole transcription.",
            "type": "boolean"
          },
          "summarization": {
            "default": false,
            "description": "If true, generates summarization for the whole transcription.",
            "type": "boolean"
          },
          "summarization_config": {
            "allOf": [
              {
                "$ref": "#/components/schemas/SummarizationConfigDTO"
              }
            ],
            "description": "Summarization configuration, if `summarization` is enabled"
          }
        },
        "type": "object"
      },
      "PreProcessingConfig": {
        "properties": {
          "audio_enhancer": {
            "default": false,
            "description": "If true, apply pre-processing to the audio stream to enhance the quality.",
            "type": "boolean"
          },
          "speech_threshold": {
            "default": 0.6,
            "description": "Sensitivity configuration for Speech Threshold. A value close to 1 will apply stricter thresholds, making it less likely to detect background sounds as speech.",
            "maximum": 1,
            "minimum": 0,
            "type": "number"
          }
        },
        "type": "object"
      },
      "RealtimeProcessingConfig": {
        "properties": {
          "custom_spelling": {
            "default": false,
            "description": "If true, enable custom spelling for the transcription.",
            "type": "boolean"
          },
          "custom_spelling_config": {
            "allOf": [
              {
                "$ref": "#/components/schemas/CustomSpellingConfigDTO"
              }
            ],
            "description": "Custom spelling configuration, if `custom_spelling` is enabled"
          },
          "custom_vocabulary": {
            "default": false,
            "description": "If true, enable custom vocabulary for the transcription.",
            "type": "boolean"
          },
          "custom_vocabulary_config": {
            "allOf": [
              {
                "$ref": "#/components/schemas/CustomVocabularyConfigDTO"
              }
            ],
            "description": "Custom vocabulary configuration, if `custom_vocabulary` is enabled"
          },
          "named_entity_recognition": {
            "default": false,
            "description": "If true, enable named entity recognition for the transcription.",
            "type": "boolean"
          },
          "sentiment_analysis": {
            "default": false,
            "description": "If true, enable sentiment analysis for the transcription.",
            "type": "boolean"
          },
          "translation": {
            "default": false,
            "description": "If true, enable translation for the transcription",
            "type": "boolean"
          },
          "translation_config": {
            "allOf": [
              {
                "$ref": "#/components/schemas/TranslationConfigDTO"
              }
            ],
            "description": "Translation configuration, if `translation` is enabled"
          }
        },
        "type": "object"
      },
      "StreamingSupportedSampleRateEnum": {
        "description": "The sample rate of the audio stream",
        "enum": [
          8000,
          16000,
          32000,
          44100,
          48000
        ],
        "type": "number"
      },
      "ChapterizationDTO": {
        "properties": {
          "error": {
            "allOf": [
              {
                "$ref": "#/components/schemas/AddonErrorDTO"
              }
            ],
            "description": "`null` if `success` is `true`. Contains the error details of the failed model",
            "nullable": true
          },
          "exec_time": {
            "description": "Time audio intelligence model took to complete the task",
            "type": "number"
          },
          "is_empty": {
            "description": "The audio intelligence model returned an empty value",
            "type": "boolean"
          },
          "results": {
            "additionalProperties": true,
            "description": "If `chapterization` has been enabled, will generate chapters name for different parts of the given audio.",
            "type": "object"
          },
          "success": {
            "description": "The audio intelligence model succeeded to get a valid output",
            "type": "boolean"
          }
        },
        "required": [
          "success",
          "is_empty",
          "exec_time",
          "error",
          "results"
        ],
        "type": "object"
      },
      "TranscriptionMetadataDTO": {
        "properties": {
          "audio_duration": {
            "description": "Duration of the transcribed audio file",
            "example": 3600,
            "type": "number"
          },
          "billing_time": {
            "description": "Billed duration in seconds (audio_duration * number_of_distinct_channels)",
            "example": 3600,
            "type": "number"
          },
          "number_of_distinct_channels": {
            "description": "Number of distinct channels in the transcribed audio file",
            "example": 1,
            "minimum": 1,
            "type": "integer"
          },
          "transcription_time": {
            "description": "Duration of the transcription in seconds",
            "example": 20,
            "type": "number"
          }
        },
        "required": [
          "audio_duration",
          "number_of_distinct_channels",
          "billing_time",
          "transcription_time"
        ],
        "type": "object"
      },
      "NamedEntityRecognitionDTO": {
        "properties": {
          "entity": {
            "description": "If `named_entity_recognition` has been enabled, the detected entities.",
            "type": "string"
          },
          "error": {
            "allOf": [
              {
                "$ref": "#/components/schemas/AddonErrorDTO"
              }
            ],
            "description": "`null` if `success` is `true`. Contains the error details of the failed model",
            "nullable": true
          },
          "exec_time": {
            "description": "Time audio intelligence model took to complete the task",
            "type": "number"
          },
          "is_empty": {
            "description": "The audio intelligence model returned an empty value",
            "type": "boolean"
          },
          "success": {
            "description": "The audio intelligence model succeeded to get a valid output",
            "type": "boolean"
          }
        },
        "required": [
          "success",
          "is_empty",
          "exec_time",
          "error",
          "entity"
        ],
        "type": "object"
      },
      "SentimentAnalysisDTO": {
        "properties": {
          "error": {
            "allOf": [
              {
                "$ref": "#/components/schemas/AddonErrorDTO"
              }
            ],
            "description": "`null` if `success` is `true`. Contains the error details of the failed model",
            "nullable": true
          },
          "exec_time": {
            "description": "Time audio intelligence model took to complete the task",
            "type": "number"
          },
          "is_empty": {
            "description": "The audio intelligence model returned an empty value",
            "type": "boolean"
          },
          "results": {
            "description": "If `sentiment_analysis` has been enabled, Gladia will analyze the sentiments and emotions of the audio",
            "type": "string"
          },
          "success": {
            "description": "The audio intelligence model succeeded to get a valid output",
            "type": "boolean"
          }
        },
        "required": [
          "success",
          "is_empty",
          "exec_time",
          "error",
          "results"
        ],
        "type": "object"
      },
      "SummarizationDTO": {
        "properties": {
          "error": {
            "allOf": [
              {
                "$ref": "#/components/schemas/AddonErrorDTO"
              }
            ],
            "description": "`null` if `success` is `true`. Contains the error details of the failed model",
            "nullable": true
          },
          "exec_time": {
            "description": "Time audio intelligence model took to complete the task",
            "type": "number"
          },
          "is_empty": {
            "description": "The audio intelligence model returned an empty value",
            "type": "boolean"
          },
          "results": {
            "description": "If `summarization` has been enabled, summary of the transcription",
            "nullable": true,
            "type": "string"
          },
          "success": {
            "description": "The audio intelligence model succeeded to get a valid output",
            "type": "boolean"
          }
        },
        "required": [
          "success",
          "is_empty",
          "exec_time",
          "error",
          "results"
        ],
        "type": "object"
      },
      "TranscriptionDTO": {
        "properties": {
          "full_transcript": {
            "description": "All transcription on text format without any other information",
            "type": "string"
          },
          "languages": {
            "description": "All the detected languages in the audio sorted from the most detected to the less detected",
            "example": [
              "en"
            ],
            "items": {
              "$ref": "#/components/schemas/TranscriptionLanguageCodeEnum"
            },
            "type": "array"
          },
          "sentences": {
            "description": "If `sentences` has been enabled, sentences results",
            "items": {
              "$ref": "#/components/schemas/SentencesDTO"
            },
            "type": "array"
          },
          "subtitles": {
            "description": "If `subtitles` has been enabled, subtitles results",
            "items": {
              "$ref": "#/components/schemas/SubtitleDTO"
            },
            "type": "array"
          },
          "utterances": {
            "description": "Transcribed speech utterances present in the audio",
            "items": {
              "$ref": "#/components/schemas/UtteranceDTO"
            },
            "type": "array"
          }
        },
        "required": [
          "full_transcript",
          "languages",
          "utterances"
        ],
        "type": "object"
      },
      "TranslationDTO": {
        "properties": {
          "error": {
            "allOf": [
              {
                "$ref": "#/components/schemas/AddonErrorDTO"
              }
            ],
            "description": "`null` if `success` is `true`. Contains the error details of the failed model",
            "nullable": true
          },
          "exec_time": {
            "description": "Time audio intelligence model took to complete the task",
            "type": "number"
          },
          "is_empty": {
            "description": "The audio intelligence model returned an empty value",
            "type": "boolean"
          },
          "results": {
            "description": "List of translated transcriptions, one for each `target_languages`",
            "items": {
              "$ref": "#/components/schemas/TranslationResultDTO"
            },
            "nullable": true,
            "type": "array"
          },
          "success": {
            "description": "The audio intelligence model succeeded to get a valid output",
            "type": "boolean"
          }
        },
        "required": [
          "success",
          "is_empty",
          "exec_time",
          "error",
          "results"
        ],
        "type": "object"
      },
      "TranscriptionLanguageCodeEnum": {
        "description": "Specify the language in which it will be pronounced when sound comparison occurs. Default to transcription language.",
        "enum": [
          "af",
          "am",
          "ar",
          "as",
          "az",
          "ba",
          "be",
          "bg",
          "bn",
          "bo",
          "br",
          "bs",
          "ca",
          "cs",
          "cy",
          "da",
          "de",
          "el",
          "en",
          "es",
          "et",
          "eu",
          "fa",
          "fi",
          "fo",
          "fr",
          "gl",
          "gu",
          "ha",
          "haw",
          "he",
          "hi",
          "hr",
          "ht",
          "hu",
          "hy",
          "id",
          "is",
          "it",
          "ja",
          "jw",
          "ka",
          "kk",
          "km",
          "kn",
          "ko",
          "la",
          "lb",
          "ln",
          "lo",
          "lt",
          "lv",
          "mg",
          "mi",
          "mk",
          "ml",
          "mn",
          "mr",
          "ms",
          "mt",
          "my",
          "ne",
          "nl",
          "nn",
          "no",
          "oc",
          "pa",
          "pl",
          "ps",
          "pt",
          "ro",
          "ru",
          "sa",
          "sd",
          "si",
          "sk",
          "sl",
          "sn",
          "so",
          "sq",
          "sr",
          "su",
          "sv",
          "sw",
          "ta",
          "te",
          "tg",
          "th",
          "tk",
          "tl",
          "tr",
          "tt",
          "uk",
          "ur",
          "uz",
          "vi",
          "yi",
          "yo",
          "zh"
        ],
        "type": "string"
      },
      "SummarizationConfigDTO": {
        "properties": {
          "type": {
            "allOf": [
              {
                "$ref": "#/components/schemas/SummaryTypesEnum"
              }
            ],
            "default": "general",
            "description": "The type of summarization to apply"
          }
        },
        "type": "object"
      },
      "CustomSpellingConfigDTO": {
        "properties": {
          "spelling_dictionary": {
            "additionalProperties": {
              "items": {
                "type": "string"
              },
              "type": "array"
            },
            "description": "The list of spelling applied on the audio transcription",
            "example": {
              "Gettleman": [
                "gettleman"
              ],
              "SQL": [
                "Sequel"
              ]
            },
            "type": "object"
          }
        },
        "required": [
          "spelling_dictionary"
        ],
        "type": "object"
      },
      "CustomVocabularyConfigDTO": {
        "properties": {
          "default_intensity": {
            "description": "Default intensity for the custom vocabulary",
            "example": 0.5,
            "maximum": 1,
            "minimum": 0,
            "type": "number"
          },
          "vocabulary": {
            "description": "Specific vocabulary list to feed the transcription model with. Each item can be a string or an object with the following properties: value, intensity, pronunciations, language.",
            "example": [
              "Westeros",
              {
                "value": "Stark"
              },
              {
                "intensity": 0.4,
                "language": "en",
                "pronunciations": [
                  "Nightz Watch"
                ],
                "value": "Night's Watch"
              }
            ],
            "items": {
              "oneOf": [
                {
                  "$ref": "#/components/schemas/CustomVocabularyEntryDTO"
                },
                {
                  "type": "string"
                }
              ]
            },
            "type": "array"
          }
        },
        "required": [
          "vocabulary"
        ],
        "type": "object"
      },
      "TranslationConfigDTO": {
        "properties": {
          "context": {
            "description": "Context information to improve translation accuracy",
            "type": "string"
          },
          "context_adaptation": {
            "default": true,
            "description": "Enables or disables context-aware translation features that allow the model to adapt translations based on provided context.",
            "type": "boolean"
          },
          "informal": {
            "default": false,
            "description": "Forces the translation to use informal language forms when available in the target language.",
            "type": "boolean"
          },
          "lipsync": {
            "default": true,
            "description": "Whether to apply lipsync to the translated transcription. ",
            "type": "boolean"
          },
          "match_original_utterances": {
            "default": true,
            "description": "Align translated utterances with the original ones",
            "type": "boolean"
          },
          "model": {
            "allOf": [
              {
                "$ref": "#/components/schemas/TranslationModelEnum"
              }
            ],
            "default": "base",
            "description": "Model you want the translation model to use to translate"
          },
          "target_languages": {
            "description": "Target language in `iso639-1` format you want the transcription translated to",
            "example": [
              "en"
            ],
            "items": {
              "$ref": "#/components/schemas/TranslationLanguageCodeEnum"
            },
            "minItems": 1,
            "type": "array"
          }
        },
        "required": [
          "target_languages"
        ],
        "type": "object"
      },
      "AddonErrorDTO": {
        "properties": {
          "exception": {
            "description": "Reason of the addon error",
            "type": "string"
          },
          "message": {
            "description": "Detailed message of the addon error",
            "type": "string"
          },
          "status_code": {
            "description": "Status code of the addon error",
            "example": 500,
            "type": "integer"
          }
        },
        "required": [
          "status_code",
          "exception",
          "message"
        ],
        "type": "object"
      },
      "SentencesDTO": {
        "properties": {
          "error": {
            "allOf": [
              {
                "$ref": "#/components/schemas/AddonErrorDTO"
              }
            ],
            "description": "`null` if `success` is `true`. Contains the error details of the failed model",
            "nullable": true
          },
          "exec_time": {
            "description": "Time audio intelligence model took to complete the task",
            "type": "number"
          },
          "is_empty": {
            "description": "The audio intelligence model returned an empty value",
            "type": "boolean"
          },
          "results": {
            "description": "If `sentences` has been enabled, transcription as sentences.",
            "items": {
              "type": "string"
            },
            "nullable": true,
            "type": "array"
          },
          "success": {
            "description": "The audio intelligence model succeeded to get a valid output",
            "type": "boolean"
          }
        },
        "required": [
          "success",
          "is_empty",
          "exec_time",
          "error",
          "results"
        ],
        "type": "object"
      },
      "SubtitleDTO": {
        "properties": {
          "format": {
            "allOf": [
              {
                "$ref": "#/components/schemas/SubtitlesFormatEnum"
              }
            ],
            "description": "Format of the current subtitle",
            "example": "srt"
          },
          "subtitles": {
            "description": "Transcription on the asked subtitle format",
            "type": "string"
          }
        },
        "required": [
          "format",
          "subtitles"
        ],
        "type": "object"
      },
      "UtteranceDTO": {
        "properties": {
          "channel": {
            "description": "Audio channel of where this utterance has been transcribed from",
            "minimum": 0,
            "type": "integer"
          },
          "confidence": {
            "description": "Confidence on the transcribed utterance (1 = 100% confident)",
            "type": "number"
          },
          "end": {
            "description": "End timestamp in seconds of this utterance",
            "type": "number"
          },
          "language": {
            "allOf": [
              {
                "$ref": "#/components/schemas/TranscriptionLanguageCodeEnum"
              }
            ],
            "description": "Spoken language in this utterance",
            "example": "en"
          },
          "speaker": {
            "description": "If `diarization` enabled, speaker identification number",
            "minimum": 0,
            "type": "integer"
          },
          "start": {
            "description": "Start timestamp in seconds of this utterance",
            "type": "number"
          },
          "text": {
            "description": "Transcription for this utterance",
            "type": "string"
          },
          "words": {
            "description": "List of words of the utterance, split by timestamp",
            "items": {
              "$ref": "#/components/schemas/WordDTO"
            },
            "type": "array"
          }
        },
        "required": [
          "start",
          "end",
          "confidence",
          "channel",
          "words",
          "text",
          "language"
        ],
        "type": "object"
      },
      "TranslationResultDTO": {
        "properties": {
          "error": {
            "allOf": [
              {
                "$ref": "#/components/schemas/AddonErrorDTO"
              }
            ],
            "description": "Contains the error details of the failed addon",
            "nullable": true
          },
          "full_transcript": {
            "description": "All transcription on text format without any other information",
            "type": "string"
          },
          "languages": {
            "description": "All the detected languages in the audio sorted from the most detected to the less detected",
            "example": [
              "en"
            ],
            "items": {
              "$ref": "#/components/schemas/TranslationLanguageCodeEnum"
            },
            "type": "array"
          },
          "sentences": {
            "description": "If `sentences` has been enabled, sentences results for this translation",
            "items": {
              "$ref": "#/components/schemas/SentencesDTO"
            },
            "type": "array"
          },
          "subtitles": {
            "description": "If `subtitles` has been enabled, subtitles results for this translation",
            "items": {
              "$ref": "#/components/schemas/SubtitleDTO"
            },
            "type": "array"
          },
          "utterances": {
            "description": "Transcribed speech utterances present in the audio",
            "items": {
              "$ref": "#/components/schemas/UtteranceDTO"
            },
            "type": "array"
          }
        },
        "required": [
          "error",
          "full_transcript",
          "languages",
          "utterances"
        ],
        "type": "object"
      },
      "SummaryTypesEnum": {
        "description": "The type of summarization to apply",
        "enum": [
          "general",
          "bullet_points",
          "concise"
        ],
        "type": "string"
      },
      "CustomVocabularyEntryDTO": {
        "properties": {
          "intensity": {
            "description": "The global intensity of the feature.",
            "example": 0.5,
            "maximum": 1,
            "minimum": 0,
            "type": "number"
          },
          "language": {
            "allOf": [
              {
                "$ref": "#/components/schemas/TranscriptionLanguageCodeEnum"
              }
            ],
            "description": "Specify the language in which it will be pronounced when sound comparison occurs. Default to transcription language.",
            "example": "en"
          },
          "pronunciations": {
            "description": "The pronunciations used in the transcription.",
            "items": {
              "type": "string"
            },
            "type": "array"
          },
          "value": {
            "description": "The text used to replace in the transcription.",
            "example": "Gladia",
            "type": "string"
          }
        },
        "required": [
          "value"
        ],
        "type": "object"
      },
      "TranslationModelEnum": {
        "description": "Model you want the translation model to use to translate",
        "enum": [
          "base",
          "enhanced"
        ],
        "type": "string"
      },
      "TranslationLanguageCodeEnum": {
        "description": "Target language in `iso639-1` format you want the transcription translated to",
        "enum": [
          "af",
          "am",
          "ar",
          "as",
          "az",
          "ba",
          "be",
          "bg",
          "bn",
          "bo",
          "br",
          "bs",
          "ca",
          "cs",
          "cy",
          "da",
          "de",
          "el",
          "en",
          "es",
          "et",
          "eu",
          "fa",
          "fi",
          "fo",
          "fr",
          "gl",
          "gu",
          "ha",
          "haw",
          "he",
          "hi",
          "hr",
          "ht",
          "hu",
          "hy",
          "id",
          "is",
          "it",
          "ja",
          "jw",
          "ka",
          "kk",
          "km",
          "kn",
          "ko",
          "la",
          "lb",
          "ln",
          "lo",
          "lt",
          "lv",
          "mg",
          "mi",
          "mk",
          "ml",
          "mn",
          "mr",
          "ms",
          "mt",
          "my",
          "ne",
          "nl",
          "nn",
          "no",
          "oc",
          "pa",
          "pl",
          "ps",
          "pt",
          "ro",
          "ru",
          "sa",
          "sd",
          "si",
          "sk",
          "sl",
          "sn",
          "so",
          "sq",
          "sr",
          "su",
          "sv",
          "sw",
          "ta",
          "te",
          "tg",
          "th",
          "tk",
          "tl",
          "tr",
          "tt",
          "uk",
          "ur",
          "uz",
          "vi",
          "wo",
          "yi",
          "yo",
          "zh"
        ],
        "type": "string"
      },
      "SubtitlesFormatEnum": {
        "description": "Subtitles formats you want your transcription to be formatted to",
        "enum": [
          "srt",
          "vtt"
        ],
        "type": "string"
      },
      "WordDTO": {
        "properties": {
          "confidence": {
            "description": "Confidence on the transcribed word (1 = 100% confident)",
            "type": "number"
          },
          "end": {
            "description": "End timestamps in seconds of the spoken word",
            "type": "number"
          },
          "start": {
            "description": "Start timestamps in seconds of the spoken word",
            "type": "number"
          },
          "word": {
            "description": "Spoken word",
            "type": "string"
          }
        },
        "required": [
          "word",
          "start",
          "end",
          "confidence"
        ],
        "type": "object"
      }
    }
  }
}
