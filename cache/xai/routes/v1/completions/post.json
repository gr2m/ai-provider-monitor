{
  "operationId": "handle_sample_request",
  "requestBody": {
    "content": {
      "application/json": {
        "example": {
          "max_tokens": 3,
          "model": "grok-3",
          "prompt": "1, 2, 3, 4, "
        },
        "schema": {
          "description": "(Legacy) Request for `/v1/completions` endpoint",
          "properties": {
            "best_of": {
              "description": "(Unsupported) Generates multiple completions internally and returns the top-scoring one. Not functional yet.",
              "format": "int32",
              "type": [
                "integer",
                "null"
              ]
            },
            "echo": {
              "description": "Option to include the original prompt in the response along with the generated completion.",
              "type": [
                "boolean",
                "null"
              ]
            },
            "frequency_penalty": {
              "description": "(Unsupported) Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.",
              "format": "float",
              "type": [
                "number",
                "null"
              ]
            },
            "logit_bias": {
              "additionalProperties": {
                "format": "float",
                "type": "number"
              },
              "description": "(Unsupported) Accepts a JSON object that maps tokens to an associated bias value from -100 to 100. You can use this tokenizer tool to convert text to token IDs. Mathematically, the bias is added to the logits generated by the model prior to sampling. The exact effect will vary per model, but values between -1 and 1 should decrease or increase likelihood of selection; values like -100 or 100 should result in a ban or exclusive selection of the relevant token.",
              "propertyNames": {
                "format": "int32",
                "type": "integer"
              },
              "type": [
                "object",
                "null"
              ]
            },
            "logprobs": {
              "description": "Include the log probabilities on the `logprobs` most likely output tokens, as well the chosen tokens. For example, if `logprobs` is 5, the API will return a list of the 5 most likely tokens. The API will always return the logprob of the sampled token, so there may be up to `logprobs+1` elements in the response.",
              "type": [
                "boolean",
                "null"
              ]
            },
            "max_tokens": {
              "description": "Limits the number of tokens that can be produced in the output. Ensure the sum of prompt tokens and `max_tokens` does not exceed the model's context limit.",
              "format": "int32",
              "type": [
                "integer",
                "null"
              ]
            },
            "model": {
              "description": "Specifies the model to be used for the request.",
              "type": "string"
            },
            "n": {
              "description": "Determines how many completion sequences to produce for each prompt. Be cautious with its use due to high token consumption; adjust `max_tokens` and stop sequences accordingly.",
              "format": "int32",
              "type": [
                "integer",
                "null"
              ]
            },
            "presence_penalty": {
              "description": "(Not supported by `grok-3` and reasoning models) Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.",
              "format": "float",
              "type": [
                "number",
                "null"
              ]
            },
            "prompt": {
              "description": "Input for generating completions, which can be a string, list of strings, token list, or list of token lists. `<|endoftext|>` is used as a document separator, implying a new context start if omitted.",
              "oneOf": [
                {
                  "description": "Text prompt.",
                  "type": "string"
                },
                {
                  "description": "An array of strings, a token list, or an array of token lists.",
                  "items": {
                    "type": "string"
                  },
                  "type": "array"
                }
              ]
            },
            "seed": {
              "description": "If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result. Determinism is not guaranteed, and you should refer to the system_fingerprint response parameter to monitor changes in the backend.",
              "format": "int32",
              "type": [
                "integer",
                "null"
              ]
            },
            "stop": {
              "description": "(Not supported by reasoning models) Up to 4 sequences where the API will stop generating further tokens.",
              "items": {
                "type": "string"
              },
              "type": [
                "array",
                "null"
              ]
            },
            "stream": {
              "description": "Whether to stream back partial progress. If set, tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a `data: [DONE]` message.",
              "type": [
                "boolean",
                "null"
              ]
            },
            "stream_options": {
              "oneOf": [
                {
                  "type": "null"
                },
                {
                  "description": "Options for streaming response. Only set this when you set `stream: true`.",
                  "properties": {
                    "include_usage": {
                      "description": "Set an additional chunk to be streamed before the `data: [DONE]` message. The other chunks will return `null` in `usage` field.",
                      "type": "boolean"
                    }
                  },
                  "required": [
                    "include_usage"
                  ],
                  "type": "object"
                }
              ]
            },
            "suffix": {
              "description": "(Unsupported) Optional string to append after the generated text.",
              "type": [
                "string",
                "null"
              ]
            },
            "temperature": {
              "default": 1,
              "description": "What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. We generally recommend altering this or `top_p` but not both.",
              "example": 0.2,
              "format": "float",
              "maximum": 2,
              "minimum": 0,
              "type": [
                "number",
                "null"
              ]
            },
            "top_p": {
              "description": "An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with `top_p` probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. We generally recommend altering this or temperature but not both.",
              "format": "float",
              "type": [
                "number",
                "null"
              ]
            },
            "user": {
              "description": "A unique identifier representing your end-user, which can help xAI to monitor and detect abuse.",
              "type": [
                "string",
                "null"
              ]
            }
          },
          "type": "object"
        }
      }
    },
    "required": true
  },
  "responses": {
    "200": {
      "content": {
        "application/json": {
          "example": {
            "choices": [
              {
                "finish_reason": "length",
                "index": 0,
                "text": "5, "
              }
            ],
            "created": 1743771779,
            "id": "873492b3-6144-4279-ac2e-2c45242c5ce6",
            "model": "grok-3",
            "object": "text_completion",
            "system_fingerprint": "fp_156d35dcaa",
            "usage": {
              "completion_tokens": 3,
              "completion_tokens_details": {
                "accepted_prediction_tokens": 0,
                "audio_tokens": 0,
                "reasoning_tokens": 0,
                "rejected_prediction_tokens": 0
              },
              "prompt_tokens": 12,
              "prompt_tokens_details": {
                "audio_tokens": 0,
                "cached_tokens": 0,
                "image_tokens": 0,
                "text_tokens": 12
              },
              "total_tokens": 15
            }
          },
          "schema": {
            "description": "(Legacy) Response for `/v1/completions` endpoint",
            "properties": {
              "choices": {
                "description": "A list of response choices from the model. The length corresponds to the `n` in request body (default to 1).",
                "items": {
                  "properties": {
                    "finish_reason": {
                      "description": "Finish reason. `\"stop\"` means the inference has reached a model-defined or user-supplied stop sequence in `stop`. `\"length\"` means the inference result has reached models' maximum allowed token length or user defined value in `max_tokens`. `\"end_turn\"` or `null` in streaming mode when the chunk is not the last.",
                      "type": "string"
                    },
                    "index": {
                      "description": "Index of the choice.",
                      "format": "int32",
                      "type": "integer"
                    },
                    "text": {
                      "description": "Text response.",
                      "type": "string"
                    }
                  },
                  "required": [
                    "index",
                    "text",
                    "finish_reason"
                  ],
                  "type": "object"
                },
                "type": "array"
              },
              "created": {
                "description": "The chat completion creation time in Unix timestamp.",
                "format": "int64",
                "type": "integer"
              },
              "id": {
                "description": "ID of the request.",
                "type": "string"
              },
              "model": {
                "description": "Model to be used.",
                "example": "grok-4",
                "type": "string"
              },
              "object": {
                "description": "Object type of the response. This is always `\"text_completion\"`.",
                "type": "string"
              },
              "system_fingerprint": {
                "description": "System fingerprint, used to indicate xAI system configuration changes.",
                "type": [
                  "string",
                  "null"
                ]
              },
              "usage": {
                "oneOf": [
                  {
                    "type": "null"
                  },
                  {
                    "description": "Token usage information.",
                    "properties": {
                      "completion_tokens": {
                        "description": "Total completion token used.",
                        "format": "int32",
                        "type": "integer"
                      },
                      "completion_tokens_details": {
                        "description": "Breakdown of completion token usage of different types.",
                        "properties": {
                          "accepted_prediction_tokens": {
                            "description": "The number of tokens in the prediction that appeared in the completion.",
                            "format": "int32",
                            "type": "integer"
                          },
                          "audio_tokens": {
                            "description": "Audio input tokens generated by the model.",
                            "format": "int32",
                            "type": "integer"
                          },
                          "reasoning_tokens": {
                            "description": "Tokens generated by the model for reasoning.",
                            "format": "int32",
                            "type": "integer"
                          },
                          "rejected_prediction_tokens": {
                            "description": "The number of tokens in the prediction that did not appear in the completion.",
                            "format": "int32",
                            "type": "integer"
                          }
                        },
                        "required": [
                          "reasoning_tokens",
                          "audio_tokens",
                          "accepted_prediction_tokens",
                          "rejected_prediction_tokens"
                        ],
                        "type": "object"
                      },
                      "num_sources_used": {
                        "description": "Number of individual live search source used.",
                        "format": "int32",
                        "type": "integer"
                      },
                      "prompt_tokens": {
                        "description": "Total prompt token used.",
                        "format": "int32",
                        "type": "integer"
                      },
                      "prompt_tokens_details": {
                        "description": "Breakdown of prompt token usage of different types.",
                        "properties": {
                          "audio_tokens": {
                            "description": "Audio prompt token used.",
                            "format": "int32",
                            "type": "integer"
                          },
                          "cached_tokens": {
                            "description": "Token cached by xAI from previous requests and reused for this request.",
                            "format": "int32",
                            "type": "integer"
                          },
                          "image_tokens": {
                            "description": "Image prompt token used.",
                            "format": "int32",
                            "type": "integer"
                          },
                          "text_tokens": {
                            "description": "Text prompt token used.",
                            "format": "int32",
                            "type": "integer"
                          }
                        },
                        "required": [
                          "text_tokens",
                          "audio_tokens",
                          "image_tokens",
                          "cached_tokens"
                        ],
                        "type": "object"
                      },
                      "total_tokens": {
                        "description": "Total token used, the sum of prompt token and completion token amount.",
                        "format": "int32",
                        "type": "integer"
                      }
                    },
                    "required": [
                      "prompt_tokens",
                      "completion_tokens",
                      "total_tokens",
                      "prompt_tokens_details",
                      "completion_tokens_details",
                      "num_sources_used"
                    ],
                    "type": "object"
                  }
                ]
              }
            },
            "required": [
              "id",
              "object",
              "created",
              "model",
              "choices"
            ],
            "type": "object"
          }
        }
      },
      "description": "Success"
    },
    "400": {
      "description": "Bad request. The request is invalid or an invalid API key is provided."
    },
    "422": {
      "description": "Unprocessable Entity. There are missing fields in the request body."
    }
  },
  "security": [
    {
      "bearerAuth": []
    }
  ],
  "summary": "(Legacy - Not supported by reasoning models) Create a text completion response for a given prompt. Replaced by /v1/chat/completions.",
  "tags": [
    "v1"
  ]
}
