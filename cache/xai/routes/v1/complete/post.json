{
  "operationId": "handle_generic_complete_request",
  "requestBody": {
    "content": {
      "application/json": {
        "example": {
          "max_tokens_to_sample": 8,
          "model": "grok-3",
          "prompt": "\n\nHuman: Hello, how are you?\n\nAssistant:",
          "temperature": 0.1
        },
        "schema": {
          "description": "(Legacy) Anthropic compatible complete request on `/v1/complete` endpoint.",
          "properties": {
            "max_tokens_to_sample": {
              "description": "The maximum number of tokens to generate before stopping.",
              "format": "int32",
              "type": "integer"
            },
            "metadata": {
              "oneOf": [
                {
                  "type": "null"
                },
                {
                  "description": "An object describing metadata about the request.",
                  "properties": {
                    "user_id": {
                      "description": "A unique identifier representing your end-user, which can help xAI to monitor and detect abuse.",
                      "type": [
                        "string",
                        "null"
                      ]
                    }
                  },
                  "type": "object"
                }
              ]
            },
            "model": {
              "description": "Model to use for completion.",
              "type": "string"
            },
            "prompt": {
              "description": "Prompt for the model to perform completion on.",
              "type": "string"
            },
            "stop_sequences": {
              "description": "(Not supported by reasoning models) Up to 4 sequences where the API will stop generating further tokens.",
              "items": {
                "type": "string"
              },
              "type": [
                "array",
                "null"
              ]
            },
            "stream": {
              "description": "(Unsupported) If set, partial message deltas will be sent. Tokens will be sent as data-only server-sent events as they become available, with the stream terminated by a `data: [DONE]` message.",
              "type": [
                "boolean",
                "null"
              ]
            },
            "temperature": {
              "default": 1,
              "description": "What sampling temperature to use, between 0 and 2. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.",
              "example": 0.2,
              "format": "float",
              "maximum": 2,
              "minimum": 0,
              "type": [
                "number",
                "null"
              ]
            },
            "top_k": {
              "description": "(Unsupported) When generating next tokens, randomly selecting the next token from the k most likely options.",
              "format": "int32",
              "type": [
                "integer",
                "null"
              ]
            },
            "top_p": {
              "default": 1,
              "description": "An alternative to sampling with `temperature`, called nucleus sampling, where the model considers the results of the tokens with `top_p` probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. It is generally recommended to alter this or `temperature` but not both.",
              "exclusiveMinimum": 0,
              "format": "float",
              "maximum": 1,
              "type": [
                "number",
                "null"
              ]
            }
          },
          "type": "object"
        }
      }
    },
    "required": true
  },
  "responses": {
    "200": {
      "content": {
        "application/json": {
          "example": {
            "completion": " Hey there! I'm doing great, thanks",
            "id": "982044c5-760c-4c8d-8936-f906b5cedc26",
            "model": "grok-3",
            "stop_reason": "max_tokens",
            "type": "completion"
          },
          "schema": {
            "description": "(Legacy) Anthropic compatible complete response on `/v1/complete` endpoint.",
            "properties": {
              "completion": {
                "description": "The completion content up to and excluding stop sequences.",
                "type": "string"
              },
              "id": {
                "description": "ID of the completion response.",
                "type": "string"
              },
              "model": {
                "description": "The model that handled the request.",
                "type": "string"
              },
              "stop_reason": {
                "description": "The reason to stop completion. `\"stop_sequence\"` means the inference has reached a model-defined or user-supplied stop sequence in `stop`. `\"length\"` means the inference result has reached models' maximum allowed token length or user defined value in `max_tokens`. `\"end_turn\"` or `null` in streaming mode when the chunk is not the last.",
                "type": [
                  "string",
                  "null"
                ]
              },
              "type": {
                "description": "Completion response object type. This is always `\"completion\"`.",
                "type": "string"
              }
            },
            "required": [
              "type",
              "id",
              "completion",
              "model"
            ],
            "type": "object"
          }
        }
      },
      "description": "Success"
    },
    "400": {
      "description": "Bad request. The request is invalid or an invalid API key is provided."
    },
    "422": {
      "description": "Unprocessable Entity. There are missing fields in the request body."
    }
  },
  "security": [
    {
      "bearerAuth": []
    }
  ],
  "summary": "(Legacy - Not supported by reasoning models) Create a text completion response. This endpoint is compatible with the Anthropic API.",
  "tags": [
    "v1"
  ]
}
